{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "> Collection of utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# hide\n",
    "from fastai.basics import *\n",
    "from fastai.callbacks import GeneralScheduler, TrainingPhase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def param_parser(func):\n",
    "    \"Look at params in func and return an dict of args: default\"\n",
    "    p ={}\n",
    "    params = inspect.signature(func).parameters\n",
    "    for k, v in params.items():\n",
    "        if v.default != inspect.Parameter.empty: p[k] = v.default\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.001,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_parser(optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lr finder and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def lr_f(learn):\n",
    "    learn.lr_find()\n",
    "    learn.recorder.plot()\n",
    "    learn.p.min_lr = learn.recorder.lrs[learn.recorder.losses.index(min(learn.recorder.losses))] # todo check if learn.p exist\n",
    "    print(f'min lr: {learn.p.min_lr:0.4f}')\n",
    "# Learner.lr_f = lr_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def plot(learn, plot_all=False):\n",
    "    learn.recorder.plot_metrics()\n",
    "    learn.recorder.plot_losses()\n",
    "    if plot_all:\n",
    "        learn.recorder.plot()\n",
    "        learn.recorder.plot_lr(show_moms=True)\n",
    "Learner.plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Basic train \n",
    "# Train learn with fit_one_cycle from fastai\n",
    "def train(learn, show_graph=False):\n",
    "    '''Run fit_one_cycle with learner, params in learn.p'''\n",
    "    cbs = ShowGraph(learn) if show_graph else None\n",
    "    print(f\"epochs: {learn.p.epochs}, lr: {learn.p.lr:0.4f}, opt - {learn.p.opt_name}\")\n",
    "    learn.fit_one_cycle(learn.p.epochs, max_lr=learn.p.lr, div_factor=learn.p.div_factor, \n",
    "                        moms=learn.p.moms, pct_start=learn.p.pct_start, final_div=learn.p.final_div, callbacks=cbs)\n",
    "Learner.train=train    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC from fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Train learn with fit_fc from fastai.\n",
    "def train_fc(learn, show_graph=False):\n",
    "    '''Run fit_fc with learn, paramas in learn.p'''\n",
    "    cbs = ShowGraph(learn) if show_graph else None\n",
    "    print(f\"epochs: {learn.p.epochs}, lr: {learn.p.lr:0.4f}, opt - {learn.p.opt_name}\")\n",
    "    learn.fit_fc(learn.p.epochs, lr=learn.p.lr, start_pct=learn.p.fc_start_pct, \n",
    "                        moms=learn.p.moms, callbacks=cbs)\n",
    "# Learner.train_fc=train_fc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC with refactoring. Lr and MOM separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Callback for Lr sheduller\n",
    "def FCSchedLr(learn, tot_epochs:int=1,  lr:float=4e-3,  start_pct:float=0.72, \n",
    "            lr_final_div=None, curve='cosine'):\n",
    "    \"\"\"refactored version of FCFit from fastai, only Lr\"\"\"\n",
    "    b_all = len(learn.data.train_dl) * tot_epochs\n",
    "    b_flat = int(b_all * start_pct)\n",
    "    b_reduce = b_all - b_flat\n",
    "    curve_type=annealing_cos\n",
    "#     if curve==\"cosine\":        curve_type=annealing_cos\n",
    "#     elif curve==\"linear\":      curve_type=annealing_linear\n",
    "#     elif curve==\"exponential\": curve_type=annealing_exp\n",
    "#     else: raiseValueError(f\"annealing type not supported {curve}\")\n",
    "\n",
    "    ph_lr_flat = TrainingPhase(b_flat).schedule_hp('lr', lr)\n",
    "    lr = (lr, lr/lr_final_div) if lr_final_div else lr\n",
    "    ph_lr_reduce = TrainingPhase(b_reduce).schedule_hp('lr', lr, anneal=curve_type)\n",
    "    phases = [ph_lr_flat, ph_lr_reduce]\n",
    "    return GeneralScheduler(learn, phases=phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOM shcheduller\n",
    "def FCSchedMom(learn, tot_epochs:int=1, moms:Floats=(0.95,0.999),\n",
    "                          start_pct:float=0.72, shift=0.1, curve='cosine'):\n",
    "    \"\"\"refactored version of FCFit from fastai, only Lr\"\"\"\n",
    "    shift_n = start_pct*shift\n",
    "    b_all = len(learn.data.train_dl) * tot_epochs\n",
    "    b_flat = int(b_all * (start_pct-shift_n))\n",
    "    b_reduce = int(b_all*shift_n*2)\n",
    "    b_finish = b_all - b_flat - b_reduce\n",
    "        \n",
    "    \n",
    "    if curve==\"cosine\":        curve_type=annealing_cos\n",
    "    elif curve==\"linear\":      curve_type=annealing_linear\n",
    "    elif curve==\"exponential\": curve_type=annealing_exp\n",
    "    else: raiseValueError(f\"annealing type not supported {curve}\")\n",
    "\n",
    "    ph_flat = TrainingPhase(b_flat).schedule_hp('mom', moms[0])\n",
    "    ph_reduce = TrainingPhase(b_reduce).schedule_hp('mom', (moms[0],moms[1]),anneal=curve_type)\n",
    "    ph_finish = TrainingPhase(b_finish).schedule_hp('mom', moms[1])\n",
    "    phases = [ph_flat, ph_reduce, ph_finish]\n",
    "    return GeneralScheduler(learn, phases=phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# version with step\n",
    "def FCSchedMom(learn, tot_epochs:int=1, moms:Floats=(0.95,0.999),\n",
    "                          start_pct:float=0.72, shift=0.1, curve='cosine'):\n",
    "    \"\"\"refactored version of FCFit from fastai, only Lr\"\"\"\n",
    "    shift_n = start_pct*shift\n",
    "    b_all = len(learn.data.train_dl) * tot_epochs\n",
    "    print('curve - ', curve)\n",
    "    if curve==\"cosine\":\n",
    "        curve_type=annealing_cos\n",
    "        b_flat = int(b_all * (start_pct-shift_n))\n",
    "        b_reduce = int(b_all*shift_n*2)\n",
    "        b_finish = b_all - b_flat - b_reduce\n",
    "        ph_flat = TrainingPhase(b_flat).schedule_hp('mom', moms[0])\n",
    "        ph_reduce = TrainingPhase(b_reduce).schedule_hp('mom', (moms[0],moms[1]),anneal=curve_type)\n",
    "        ph_finish = TrainingPhase(b_finish).schedule_hp('mom', moms[1])\n",
    "        phases = [ph_flat, ph_reduce, ph_finish]\n",
    "    \n",
    "    elif curve==\"step\":\n",
    "#         curve_type=annealing_linear\n",
    "#     elif curve==\"exponential\": curve_type=annealing_exp\n",
    "        b_flat = int(b_all * (start_pct-shift_n))\n",
    "#         b_reduce = int(b_all*shift_n*2)\n",
    "        b_finish = b_all - b_flat\n",
    "        ph_flat = TrainingPhase(b_flat).schedule_hp('mom', moms[0])\n",
    "#         ph_reduce = TrainingPhase(b_reduce).schedule_hp('mom', (moms[0],moms[1]),anneal=curve_type)\n",
    "        ph_finish = TrainingPhase(b_finish).schedule_hp('mom', moms[1])\n",
    "        phases = [ph_flat, ph_finish]\n",
    "    \n",
    "    \n",
    "    \n",
    "    else: raiseValueError(f\"annealing type not supported {curve}\")\n",
    "\n",
    "    \n",
    "    return GeneralScheduler(learn, phases=phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# train model with schedullers - saparate Lr and MOMs\n",
    "def fc_lr_mom(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms=False, start_pct:float=0.72,\n",
    "                  wd:float=None, lr_final_div=None, shift=0.1, \n",
    "                  curve='cosine', callbacks:Optional[CallbackList]=None)->None:\n",
    "    \"Fit a model with Flat Cosine Annealing\"\n",
    "    max_lr = learn.lr_range(lr)\n",
    "    callbacks = listify(callbacks)\n",
    "    callbacks.append(FCSchedLr(learn, tot_epochs=tot_epochs,  lr=lr, start_pct=start_pct, lr_final_div=lr_final_div,curve=curve))\n",
    "    if moms: callbacks.append(FCSchedMom(learn,  tot_epochs=tot_epochs, moms=moms, start_pct=start_pct,shift=shift,curve=curve))\n",
    "    learn.fit(tot_epochs, max_lr, wd=wd, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# version for use with experiment. \n",
    "# TODO - refactor exp for automate it.\n",
    "def fc(learn, show_graph=False):\n",
    "    '''Run fc scheduled lr, mom, with learn, paramas in learn.p'''\n",
    "    cbs = ShowGraph(learn) if show_graph else None\n",
    "    print(f\"epochs: {learn.p.epochs}, lr: {learn.p.lr:0.4f}, opt - {learn.p.opt_name}\")\n",
    "    fc_lr_mom(learn, learn.p.epochs, lr=learn.p.lr, \n",
    "                    start_pct=learn.p.fc_start_pct, lr_final_div=learn.p.final_div,\n",
    "                        moms=learn.p.moms, shift=learn.p.shift, callbacks=cbs) # !wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_experiment.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_consistency.ipynb.\n",
      "Converted 03_data_imagenette.ipynb.\n",
      "Converted 50_ranger.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
